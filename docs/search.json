[
  {
    "objectID": "regex.html",
    "href": "regex.html",
    "title": "Очистка текста с помощью регулярных выражений",
    "section": "",
    "text": "Напишем функцию, которая идет по всем папкам с картинками-текстами внутри папки Corpus, распознает их функцией ocr() с помощью модели FRA. Дальше написанная нами функция чистит текст регулярными выражениями по указанным параметрам.\n\nrecognize_and_clean &lt;- function(directory) {\n  files_Jac &lt;- list.files(path = directory, pattern = \"png\", full.names = TRUE)\n  Jac_recognized &lt;- map(files_Jac, ocr, engine = tesseract('fra'))\n  text_conc_Jac &lt;- paste(Jac_recognized, collapse = \" \")\n  \n  text_conc_Jac_clean &lt;- text_conc_Jac |&gt; \n    # удаляет переносы\n    str_replace_all(\"(?&lt;=\\\\p{L})-\\\\s*\\\\n\\\\s*(?=\\\\p{L})\",'') |&gt;\n    # удаляет пробел+двоеточие\n    str_replace_all('\\\\s+:', '') |&gt; \n    # удаляет пробел + черту вертикальную\n    str_replace_all('\\\\s+\\\\|', '') |&gt; \n    # удаляет слова из водяного знака\n    str_replace_all(\"(?im)^.*\\\\b\\\\w*sca\\\\w*\\\\b.*$\",'') |&gt; \n    # удаляет цифры\n    str_replace_all('[[:digit:]]', '') |&gt; \n    # удаляем строки, в которых только один символ, потому что видим, что таких много \n    str_replace_all('(?m)^\\\\s*\\\\S\\\\s*$', '') |&gt; \n    # удаляет переносы построчные, превращает их в пробелы\n    str_replace_all(\"\\n\", \" \") |&gt;\n    # схлопывает множественные пробелы до одного\n    str_replace_all(\" +\", \" \") \n}\n\nCоздаем список папок с текстами, идем по ним, распознаем тексты-картинки с французской моделькой написанной функцией, складываем в отдельные текстовые файлы.\n\njaccottet_dirs &lt;- list.files('./corpus', full.names=TRUE)\n\nПроходим мапом, возвращается символьный вектор длиной 10, где каждый элемент - отдельный текст корпуса склеенный и очищенный:\n\ndata &lt;- map(jaccottet_dirs, recognize_and_clean)\n\nМы подготовили тексты, теперь возьмем названия папок с текстами-картинками и создадим тиббл, в котором будут в одном столбце названия произведений, в другом - сами произведения. После этого почистим названия произведений.\n\njaccottet_tibble &lt;- tibble(title = jaccottet_dirs, text = data)\njaccotet_tibble_correct_titles &lt;- jaccottet_tibble |&gt; \n  mutate(title = str_remove(title, \"./corpus/\"))"
  },
  {
    "objectID": "regex.html#сам-код",
    "href": "regex.html#сам-код",
    "title": "Очистка текста с помощью регулярных выражений",
    "section": "",
    "text": "Напишем функцию, которая идет по всем папкам с картинками-текстами внутри папки Corpus, распознает их функцией ocr() с помощью модели FRA. Дальше написанная нами функция чистит текст регулярными выражениями по указанным параметрам.\n\nrecognize_and_clean &lt;- function(directory) {\n  files_Jac &lt;- list.files(path = directory, pattern = \"png\", full.names = TRUE)\n  Jac_recognized &lt;- map(files_Jac, ocr, engine = tesseract('fra'))\n  text_conc_Jac &lt;- paste(Jac_recognized, collapse = \" \")\n  \n  text_conc_Jac_clean &lt;- text_conc_Jac |&gt; \n    # удаляет переносы\n    str_replace_all(\"(?&lt;=\\\\p{L})-\\\\s*\\\\n\\\\s*(?=\\\\p{L})\",'') |&gt;\n    # удаляет пробел+двоеточие\n    str_replace_all('\\\\s+:', '') |&gt; \n    # удаляет пробел + черту вертикальную\n    str_replace_all('\\\\s+\\\\|', '') |&gt; \n    # удаляет слова из водяного знака\n    str_replace_all(\"(?im)^.*\\\\b\\\\w*sca\\\\w*\\\\b.*$\",'') |&gt; \n    # удаляет цифры\n    str_replace_all('[[:digit:]]', '') |&gt; \n    # удаляем строки, в которых только один символ, потому что видим, что таких много \n    str_replace_all('(?m)^\\\\s*\\\\S\\\\s*$', '') |&gt; \n    # удаляет переносы построчные, превращает их в пробелы\n    str_replace_all(\"\\n\", \" \") |&gt;\n    # схлопывает множественные пробелы до одного\n    str_replace_all(\" +\", \" \") \n}\n\nCоздаем список папок с текстами, идем по ним, распознаем тексты-картинки с французской моделькой написанной функцией, складываем в отдельные текстовые файлы.\n\njaccottet_dirs &lt;- list.files('./corpus', full.names=TRUE)\n\nПроходим мапом, возвращается символьный вектор длиной 10, где каждый элемент - отдельный текст корпуса склеенный и очищенный:\n\ndata &lt;- map(jaccottet_dirs, recognize_and_clean)\n\nМы подготовили тексты, теперь возьмем названия папок с текстами-картинками и создадим тиббл, в котором будут в одном столбце названия произведений, в другом - сами произведения. После этого почистим названия произведений.\n\njaccottet_tibble &lt;- tibble(title = jaccottet_dirs, text = data)\njaccotet_tibble_correct_titles &lt;- jaccottet_tibble |&gt; \n  mutate(title = str_remove(title, \"./corpus/\"))"
  },
  {
    "objectID": "frequency.knit.html",
    "href": "frequency.knit.html",
    "title": "Лемматизированный словарь частотностей",
    "section": "",
    "text": "В этом разделе мы показываем, как была выполнена лемматизация с помощью пакета udpipe и посчитаны самые частотные для словаря Жакоте леммы.\n\n\n\n\nlibrary(qpdf) library(pdftools) library(magick) library(purrr) library(tidyverse) library(tidytext) library(udpipe) library(tesseract)\n\nlibrary(qpdf)\nlibrary(pdftools)\n\nUsing poppler version 25.09.1\n\nlibrary(magick)\n\nLinking to ImageMagick 6.9.13.29\nEnabled features: cairo, fontconfig, freetype, heic, lcms, pango, raw, rsvg, webp\nDisabled features: fftw, ghostscript, x11\n\nlibrary(purrr)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.2\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidytext)\nlibrary(udpipe)\nlibrary(tesseract)\n\n\n\n\n\nudpipe_download_model(language = \"french-gsd\")\n\n    language                                                      file_model\n1 french-gsd /Users/elina/Desktop/RPROGR/JJJ/french-gsd-ud-2.5-191206.udpipe\n                                                                                                                                url\n1 https://raw.githubusercontent.com/jwijffels/udpipe.models.ud.2.5/master/inst/udpipe-ud-2.5-191206/french-gsd-ud-2.5-191206.udpipe\n  download_failed download_message\n1           FALSE               OK\n\n\n\n\n\n\nfrench_gsd &lt;- udpipe_load_model(file = \"french-gsd-ud-2.5-191206.udpipe\")\ncharacter_text &lt;- as.character(jaccotet_tibble_correct_titles$text)\ncharacter_title &lt;- as.character(jaccotet_tibble_correct_titles$title)\n\njaccottet_ann &lt;- udpipe_annotate(french_gsd, character_text, doc_id = character_title)\nanno_tbl &lt;- as_tibble(jaccottet_ann)\n\n\n\n\n\nlibrary(stopwords)\nsw &lt;- stopwords(\"fr\")\nanno_without_sw &lt;- anno_tbl|&gt; \n  filter(!lemma %in% sw)\n\nlemm_clean_withput_punct &lt;- anno_without_sw |&gt; \n  filter(!upos =='PUNCT')\n\n\n\n\n\nupos_tags &lt;- lemm_clean_withput_punct |&gt; \n  select(upos) |&gt; \n  unique()\n\nfinal_tibble &lt;- lemm_clean_withput_punct |&gt; \n  filter(!upos %in% c(\"PROPN\", \"PRON\", \"DET\", \"SCONJ\", \"CCONJ\", \"INTJ\", \"SYM\",\"X\", 'ADP'))\n\n\n\n\n\njaccottet_word_counts &lt;- final_tibble  |&gt; \n  count(lemma, sort = TRUE) \n\n\n\n\n\nfinal_tibble &lt;- final_tibble |&gt; \n  filter(!lemma %in% c('avoir', 'être', 'où', 'plus', 'tout', 'encore', 'autre', 'faire'))\njaccottet_word_counts_2 &lt;- final_tibble |&gt; \n  count(lemma, sort = TRUE)\n\n\n\n\n\njaccottet_word_counts_2 |&gt; \n  slice_head(n = 30) |&gt; \n  ggplot(aes(reorder(lemma, n), n, fill = lemma)) +\n  geom_col(show.legend = F) + \n  coord_flip() +\n  theme_light()"
  },
  {
    "objectID": "frequency.knit.html#проводим-лемматизацию",
    "href": "frequency.knit.html#проводим-лемматизацию",
    "title": "Лемматизированный словарь частотностей",
    "section": "",
    "text": "library(qpdf) library(pdftools) library(magick) library(purrr) library(tidyverse) library(tidytext) library(udpipe) library(tesseract)\n\nlibrary(qpdf)\nlibrary(pdftools)\n\nUsing poppler version 25.09.1\n\nlibrary(magick)\n\nLinking to ImageMagick 6.9.13.29\nEnabled features: cairo, fontconfig, freetype, heic, lcms, pango, raw, rsvg, webp\nDisabled features: fftw, ghostscript, x11\n\nlibrary(purrr)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.2\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidytext)\nlibrary(udpipe)\nlibrary(tesseract)\n\n\n\n\n\nudpipe_download_model(language = \"french-gsd\")\n\n    language                                                      file_model\n1 french-gsd /Users/elina/Desktop/RPROGR/JJJ/french-gsd-ud-2.5-191206.udpipe\n                                                                                                                                url\n1 https://raw.githubusercontent.com/jwijffels/udpipe.models.ud.2.5/master/inst/udpipe-ud-2.5-191206/french-gsd-ud-2.5-191206.udpipe\n  download_failed download_message\n1           FALSE               OK\n\n\n\n\n\n\nfrench_gsd &lt;- udpipe_load_model(file = \"french-gsd-ud-2.5-191206.udpipe\")\ncharacter_text &lt;- as.character(jaccotet_tibble_correct_titles$text)\ncharacter_title &lt;- as.character(jaccotet_tibble_correct_titles$title)\n\njaccottet_ann &lt;- udpipe_annotate(french_gsd, character_text, doc_id = character_title)\nanno_tbl &lt;- as_tibble(jaccottet_ann)\n\n\n\n\n\nlibrary(stopwords)\nsw &lt;- stopwords(\"fr\")\nanno_without_sw &lt;- anno_tbl|&gt; \n  filter(!lemma %in% sw)\n\nlemm_clean_withput_punct &lt;- anno_without_sw |&gt; \n  filter(!upos =='PUNCT')\n\n\n\n\n\nupos_tags &lt;- lemm_clean_withput_punct |&gt; \n  select(upos) |&gt; \n  unique()\n\nfinal_tibble &lt;- lemm_clean_withput_punct |&gt; \n  filter(!upos %in% c(\"PROPN\", \"PRON\", \"DET\", \"SCONJ\", \"CCONJ\", \"INTJ\", \"SYM\",\"X\", 'ADP'))\n\n\n\n\n\njaccottet_word_counts &lt;- final_tibble  |&gt; \n  count(lemma, sort = TRUE) \n\n\n\n\n\nfinal_tibble &lt;- final_tibble |&gt; \n  filter(!lemma %in% c('avoir', 'être', 'où', 'plus', 'tout', 'encore', 'autre', 'faire'))\njaccottet_word_counts_2 &lt;- final_tibble |&gt; \n  count(lemma, sort = TRUE)\n\n\n\n\n\njaccottet_word_counts_2 |&gt; \n  slice_head(n = 30) |&gt; \n  ggplot(aes(reorder(lemma, n), n, fill = lemma)) +\n  geom_col(show.legend = F) + \n  coord_flip() +\n  theme_light()"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Распознавание текста",
    "section": "",
    "text": "Мы сделали пдф-сканы физического носителя (книги). Попытались распознать текст из пдф с помощью пакета tesseract, но столкнулись с ошибкой: такой пдф не получилось распознать с помощью функции pdf_ocr_text(), поэтому мы разделили наши пдф-файлы на картинки в формате png и применили к ним функцию ocr() из пакета tesseract.\n\n\n\nlibrary(qpdf)\nlibrary(pdftools)\nlibrary(magick)\nlibrary(purrr)\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(udpipe)\nlibrary(tesseract)\n\n# 1: качаем нужный движок tesseract - французский, потому что наши тексты на французском языке\ntesseract_download(\"fra\")\n\n\n\n\nПроцесс распознавания текста мы поместили внутрь функции, которая, кроме того, чистит текст с помощью регулярок - см. страницу Regex."
  },
  {
    "objectID": "about.html#демонстрация-кода",
    "href": "about.html#демонстрация-кода",
    "title": "Распознавание текста",
    "section": "",
    "text": "library(qpdf)\nlibrary(pdftools)\nlibrary(magick)\nlibrary(purrr)\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(udpipe)\nlibrary(tesseract)\n\n# 1: качаем нужный движок tesseract - французский, потому что наши тексты на французском языке\ntesseract_download(\"fra\")"
  },
  {
    "objectID": "about.html#следующие-действия",
    "href": "about.html#следующие-действия",
    "title": "Распознавание текста",
    "section": "",
    "text": "Процесс распознавания текста мы поместили внутрь функции, которая, кроме того, чистит текст с помощью регулярок - см. страницу Regex."
  },
  {
    "objectID": "frequency.html",
    "href": "frequency.html",
    "title": "Лемматизированный словарь частотностей",
    "section": "",
    "text": "В этом разделе мы показываем, как была выполнена лемматизация с помощью пакета udpipe и посчитаны самые частотные для словаря Жакоте леммы.\n\n\n\n\nlibrary(qpdf) library(pdftools) library(magick) library(purrr) library(tidyverse) library(tidytext) library(udpipe) library(tesseract)\n\nlibrary(qpdf)\nlibrary(pdftools)\n\nUsing poppler version 25.09.1\n\nlibrary(magick)\n\nLinking to ImageMagick 6.9.13.29\nEnabled features: cairo, fontconfig, freetype, heic, lcms, pango, raw, rsvg, webp\nDisabled features: fftw, ghostscript, x11\n\nlibrary(purrr)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.2\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidytext)\nlibrary(udpipe)\nlibrary(tesseract)\n\n\n\n\n\nudpipe_download_model(language = \"french-gsd\")\n\n    language                                                      file_model\n1 french-gsd /Users/elina/Desktop/RPROGR/JJJ/french-gsd-ud-2.5-191206.udpipe\n                                                                                                                                url\n1 https://raw.githubusercontent.com/jwijffels/udpipe.models.ud.2.5/master/inst/udpipe-ud-2.5-191206/french-gsd-ud-2.5-191206.udpipe\n  download_failed download_message\n1           FALSE               OK\n\n\n\n\n\n\nfrench_gsd &lt;- udpipe_load_model(file = \"french-gsd-ud-2.5-191206.udpipe\")\ncharacter_text &lt;- as.character(jaccotet_tibble_correct_titles$text)\ncharacter_title &lt;- as.character(jaccotet_tibble_correct_titles$title)\n\njaccottet_ann &lt;- udpipe_annotate(french_gsd, character_text, doc_id = character_title)\nanno_tbl &lt;- as_tibble(jaccottet_ann)\n\n\n\n\n\nlibrary(stopwords)\nsw &lt;- stopwords(\"fr\")\nanno_without_sw &lt;- anno_tbl|&gt; \n  filter(!lemma %in% sw)\n\nlemm_clean_withput_punct &lt;- anno_without_sw |&gt; \n  filter(!upos =='PUNCT')\n\n\n\n\n\nupos_tags &lt;- lemm_clean_withput_punct |&gt; \n  select(upos) |&gt; \n  unique()\n\nfinal_tibble &lt;- lemm_clean_withput_punct |&gt; \n  filter(!upos %in% c(\"PROPN\", \"PRON\", \"DET\", \"SCONJ\", \"CCONJ\", \"INTJ\", \"SYM\",\"X\", 'ADP'))\n\n\n\n\n\njaccottet_word_counts &lt;- final_tibble  |&gt; \n  count(lemma, sort = TRUE) \n\n\n\n\n\nfinal_tibble &lt;- final_tibble |&gt; \n  filter(!lemma %in% c('avoir', 'être', 'où', 'plus', 'tout', 'encore', 'autre', 'faire'))\njaccottet_word_counts_2 &lt;- final_tibble |&gt; \n  count(lemma, sort = TRUE)\n\n\n\n\n\njaccottet_word_counts_2 |&gt; \n  slice_head(n = 30) |&gt; \n  ggplot(aes(reorder(lemma, n), n, fill = lemma)) +\n  geom_col(show.legend = F) + \n  coord_flip() +\n  theme_light()"
  },
  {
    "objectID": "frequency.html#проводим-лемматизацию",
    "href": "frequency.html#проводим-лемматизацию",
    "title": "Лемматизированный словарь частотностей",
    "section": "",
    "text": "library(qpdf) library(pdftools) library(magick) library(purrr) library(tidyverse) library(tidytext) library(udpipe) library(tesseract)\n\nlibrary(qpdf)\nlibrary(pdftools)\n\nUsing poppler version 25.09.1\n\nlibrary(magick)\n\nLinking to ImageMagick 6.9.13.29\nEnabled features: cairo, fontconfig, freetype, heic, lcms, pango, raw, rsvg, webp\nDisabled features: fftw, ghostscript, x11\n\nlibrary(purrr)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.2\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidytext)\nlibrary(udpipe)\nlibrary(tesseract)\n\n\n\n\n\nudpipe_download_model(language = \"french-gsd\")\n\n    language                                                      file_model\n1 french-gsd /Users/elina/Desktop/RPROGR/JJJ/french-gsd-ud-2.5-191206.udpipe\n                                                                                                                                url\n1 https://raw.githubusercontent.com/jwijffels/udpipe.models.ud.2.5/master/inst/udpipe-ud-2.5-191206/french-gsd-ud-2.5-191206.udpipe\n  download_failed download_message\n1           FALSE               OK\n\n\n\n\n\n\nfrench_gsd &lt;- udpipe_load_model(file = \"french-gsd-ud-2.5-191206.udpipe\")\ncharacter_text &lt;- as.character(jaccotet_tibble_correct_titles$text)\ncharacter_title &lt;- as.character(jaccotet_tibble_correct_titles$title)\n\njaccottet_ann &lt;- udpipe_annotate(french_gsd, character_text, doc_id = character_title)\nanno_tbl &lt;- as_tibble(jaccottet_ann)\n\n\n\n\n\nlibrary(stopwords)\nsw &lt;- stopwords(\"fr\")\nanno_without_sw &lt;- anno_tbl|&gt; \n  filter(!lemma %in% sw)\n\nlemm_clean_withput_punct &lt;- anno_without_sw |&gt; \n  filter(!upos =='PUNCT')\n\n\n\n\n\nupos_tags &lt;- lemm_clean_withput_punct |&gt; \n  select(upos) |&gt; \n  unique()\n\nfinal_tibble &lt;- lemm_clean_withput_punct |&gt; \n  filter(!upos %in% c(\"PROPN\", \"PRON\", \"DET\", \"SCONJ\", \"CCONJ\", \"INTJ\", \"SYM\",\"X\", 'ADP'))\n\n\n\n\n\njaccottet_word_counts &lt;- final_tibble  |&gt; \n  count(lemma, sort = TRUE) \n\n\n\n\n\nfinal_tibble &lt;- final_tibble |&gt; \n  filter(!lemma %in% c('avoir', 'être', 'où', 'plus', 'tout', 'encore', 'autre', 'faire'))\njaccottet_word_counts_2 &lt;- final_tibble |&gt; \n  count(lemma, sort = TRUE)\n\n\n\n\n\njaccottet_word_counts_2 |&gt; \n  slice_head(n = 30) |&gt; \n  ggplot(aes(reorder(lemma, n), n, fill = lemma)) +\n  geom_col(show.legend = F) + \n  coord_flip() +\n  theme_light()"
  },
  {
    "objectID": "cooc.html",
    "href": "cooc.html",
    "title": "Совместная встречаемость слов",
    "section": "",
    "text": "cooc_in_sentence &lt;- cooccurrence(final_tibble, term = \"lemma\", group = \"sentence_id\") |&gt;\n  as_tibble() |&gt;\n  filter(cooc &gt; 15)\n\n\n\n\n\nwordnetwork_in_sentence &lt;- graph_from_data_frame(cooc_in_sentence)\nggraph(wordnetwork_in_sentence, layout = \"fr\") +\n  geom_edge_arc(aes(width = cooc), alpha = 0.8, edge_colour = \"grey90\", show.legend=FALSE) +\n  geom_node_label(aes(label = name), col = \"#1f78b4\", size = 4) +\n  theme_void() +\n  labs(title = \"Совместная встречаемость слов в предложении\")\n\nWarning: The `trans` argument of `continuous_scale()` is deprecated as of ggplot2 3.5.0.\nℹ Please use the `transform` argument instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\ncooc_close_words &lt;- cooccurrence(final_tibble$lemma, skipgram = 1) |&gt; \n  as_tibble() |&gt;\n  filter(cooc &gt; 2)\n\n\n\n\n\nwordnetwork_close_words &lt;- graph_from_data_frame(cooc_close_words)\nggraph(wordnetwork_close_words, layout = \"fr\") +\n  geom_edge_arc(aes(width = cooc), alpha = 0.8, edge_colour = \"grey90\", show.legend=FALSE) +\n  geom_node_label(aes(label = name), col = \"#1f78b4\", size = 4) +\n  theme_void() +\n  labs(title = \"Совместная встречаемость слов, стоящих рядом\")"
  },
  {
    "objectID": "cooc.html#ищем-коллокации",
    "href": "cooc.html#ищем-коллокации",
    "title": "Совместная встречаемость слов",
    "section": "",
    "text": "cooc_in_sentence &lt;- cooccurrence(final_tibble, term = \"lemma\", group = \"sentence_id\") |&gt;\n  as_tibble() |&gt;\n  filter(cooc &gt; 15)\n\n\n\n\n\nwordnetwork_in_sentence &lt;- graph_from_data_frame(cooc_in_sentence)\nggraph(wordnetwork_in_sentence, layout = \"fr\") +\n  geom_edge_arc(aes(width = cooc), alpha = 0.8, edge_colour = \"grey90\", show.legend=FALSE) +\n  geom_node_label(aes(label = name), col = \"#1f78b4\", size = 4) +\n  theme_void() +\n  labs(title = \"Совместная встречаемость слов в предложении\")\n\nWarning: The `trans` argument of `continuous_scale()` is deprecated as of ggplot2 3.5.0.\nℹ Please use the `transform` argument instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\ncooc_close_words &lt;- cooccurrence(final_tibble$lemma, skipgram = 1) |&gt; \n  as_tibble() |&gt;\n  filter(cooc &gt; 2)\n\n\n\n\n\nwordnetwork_close_words &lt;- graph_from_data_frame(cooc_close_words)\nggraph(wordnetwork_close_words, layout = \"fr\") +\n  geom_edge_arc(aes(width = cooc), alpha = 0.8, edge_colour = \"grey90\", show.legend=FALSE) +\n  geom_node_label(aes(label = name), col = \"#1f78b4\", size = 4) +\n  theme_void() +\n  labs(title = \"Совместная встречаемость слов, стоящих рядом\")"
  }
]